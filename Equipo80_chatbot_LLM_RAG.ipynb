{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hVND8xY2OKY"
      },
      "source": [
        "# **Procesamiento de Lenguaje Natural**\n",
        "\n",
        "## Maestr√≠a en Inteligencia Artificial Aplicada\n",
        "#### Tecnol√≥gico de Monterrey\n",
        "#### Prof Luis Eduardo Falc√≥n Morales\n",
        "\n",
        "### **Actividad en Equipos: sistema LLM + RAG**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aimHVFOv23lm"
      },
      "source": [
        "* **Nombres y matr√≠culas:**\n",
        "\n",
        "  *   Arturo Jain Delgadillo A01794992\n",
        "  *   Ramiro Martin Jaramillo Romero A01171251\n",
        "  * Jos√© Ashamat Jaimes Saavedra A01736690\n",
        "  *   Owen J√°uregui Borb√≥n A01638122\n",
        "\n",
        "* **N√∫mero de Equipo:**\n",
        "  *   Equipo 80\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jimvsiVgjMg"
      },
      "source": [
        "* ##### **El formato de este cuaderno de Jupyter es libre, pero debe incuir al menos las siguientes secciones:**\n",
        "\n",
        "  * ##### **Introducci√≥n de la problem√°tica a resolver.**\n",
        "  * ##### **Sistema RAG + LLM**\n",
        "  * ##### **El chatbot, incluyendo ejemplos de prueba.**\n",
        "  * ##### **Conclusiones**\n",
        "\n",
        "* ##### **Pueden importar los paquetes o librer√≠as que requieran.**\n",
        "\n",
        "* ##### **Pueden incluir las celdas y l√≠neas de c√≥digo que deseen.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22lMsIlJUSoy"
      },
      "source": [
        "# Introducci√≥n de la problem√°tica a resolver."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SieQLZn0GJ3I"
      },
      "source": [
        "El personal de salud en M√©xico debe cumplir con muchas Normas Oficiales Mexicanas (NOM). Estas normas cubren higiene, bioseguridad, infraestructura y calidad. Incumplirlas puede traer multas, clausuras e incluso poner en riesgo a los pacientes.\n",
        "\n",
        "Hay tres problemas frecuentes:\n",
        "\n",
        "* Encontrar la norma correcta entre docenas de documentos y versiones.\n",
        "\n",
        "* Entender el lenguaje jur√≠dico y t√©cnico y convertirlo en pasos claros para el d√≠a a d√≠a.\n",
        "\n",
        "* Seguir las actualizaciones que publica el Diario Oficial.\n",
        "\n",
        "Cl√≠nicas peque√±as, laboratorios y comit√©s internos de calidad suelen trabajar sin apoyo legal especializado. Esto provoca retrasos en auditor√≠as y riesgos de sanci√≥n.\n",
        "\n",
        "Para cerrar esa brecha proponemos un chatbot que combina un modelo de lenguaje grande con RAG. El sistema carga 21 NOM de salud (por ejemplo la NOM-007 sobre atenci√≥n obst√©trica y la NOM-059 sobre buenas pr√°cticas de fabricaci√≥n). Cuando alguien hace una pregunta en lenguaje natural, el chatbot busca los p√°rrafos m√°s cercanos y genera una respuesta en espa√±ol con la cita original.\n",
        "\n",
        "As√≠ el personal puede:\n",
        "\n",
        "* Consultar requisitos concretos.\n",
        "\n",
        "* Recibir explicaciones sencillas con la referencia al texto oficial.\n",
        "\n",
        "* Acceder a la versi√≥n vigente sin buscar archivos PDF.\n",
        "\n",
        "Con esto mejoramos el acceso a la normativa, reducimos errores de cumplimiento y apoyamos la capacitaci√≥n continua en los servicios de salud."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQ_N7r7hUYfy"
      },
      "source": [
        "# Sistema RAG + LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bfs5Zxc9j7Uf",
        "outputId": "45ab47d4-c4cb-472b-ed0a-e5cf3e715395"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: transformers 4.53.0.dev0\n",
            "Uninstalling transformers-4.53.0.dev0:\n",
            "  Successfully uninstalled transformers-4.53.0.dev0\n",
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-5bcylz79\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-5bcylz79\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit 2166b6b4ff09f6dd3867ab982f262f66482aa968\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0.dev0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0.dev0) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0.dev0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0.dev0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0.dev0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0.dev0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0.dev0) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0.dev0) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0.dev0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.53.0.dev0) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.53.0.dev0) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.53.0.dev0) (1.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.53.0.dev0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.53.0.dev0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.53.0.dev0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.53.0.dev0) (2025.6.15)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.53.0.dev0-py3-none-any.whl size=11461635 sha256=d04cab7673c5f582d544a63ef9e5fce4649cb3838adf3ea5063dd8935d3c9d51\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-e8h2jcgl/wheels/32/4b/78/f195c684dd3a9ed21f3b39fe8f85b48df7918581b6437be143\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.52.4\n",
            "    Uninstalling transformers-4.52.4:\n",
            "      Successfully uninstalled transformers-4.52.4\n",
            "Successfully installed transformers-4.53.0.dev0\n"
          ]
        }
      ],
      "source": [
        "# Incluyan a continuaci√≥n todas las celdas (de c√≥digo o texto) que deseen...\n",
        "\n",
        "!pip uninstall -y transformers\n",
        "!pip -q install --upgrade \\\n",
        "  langchain langchain-community langchain-huggingface \\\n",
        "  faiss-cpu sentence-transformers \\\n",
        "  bitsandbytes accelerate \\\n",
        "  pypdf \"gradio>=4.17.0\"\n",
        "!pip install --no-cache-dir git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diYmiW9pNz32",
        "outputId": "2eb446e4-ea0d-44b7-bc71-20d355d33a12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os, warnings, gc, torch\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "DATA_DIR = '/content/drive/MyDrive/2025-1/NOM_PDFs'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPNz6VG2sNQN",
        "outputId": "e2868472-c424-4df6-d948-c97087cf1407"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Se leyeron 323 p√°ginas de 21 PDFs\n",
            "Total de fragmentos: 1625\n"
          ]
        }
      ],
      "source": [
        "# ---------------------------------------------------------------\n",
        "# 1. Cargar y fragmentar los PDFs\n",
        "# ---------------------------------------------------------------\n",
        "\n",
        "# LangChain es un framework de trabajo que se esta posicionando como lider en la creaci√≥n de arquitecturas/agentes\n",
        "# RAG, RAFT en forma de cadena (LangChain) y/o agentes, agentes multitooling, redes de agentes, multiagentes en\n",
        "# forma de grafo (LangGraph). Estos agentes en grafo, tienen la capacidad de elegir, con base en el query\n",
        "# sus acciones, tareas y/o pasos a seguir.\n",
        "# LangChain/LangGraph, nos brinda todas las herramientas para lograr estos procesos, e incluso\n",
        "# cuenta con LangSmith, que permite observar lo que pasa dentro del modelo (cuando esta el procesamiento)\n",
        "\n",
        "# RecursiveCharacterTextSplitter: sirve para dividir textos largos en partes m√°s peque√±as.\n",
        "# Utilizamos el framework Langchain para cargar los documentos PDF y para separar los documentos finales.\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Obtenemos la lista de archivos en la carpeta DATA_DIR que terminan en \".pdf\"\n",
        "pdf_files = [f for f in os.listdir(DATA_DIR) if f.lower().endswith('.pdf')]\n",
        "\n",
        "# Si no se encontr√≥ ning√∫n archivo PDF, mostramos un error.\n",
        "if not pdf_files:\n",
        "    raise ValueError(f\"No se encontraron PDFs en {DATA_DIR}.\")\n",
        "\n",
        "# Creamos una lista de \"lectores de PDF\", uno por cada archivo encontrado.\n",
        "loaders = [PyPDFLoader(os.path.join(DATA_DIR, f)) for f in pdf_files]\n",
        "\n",
        "# Leemos todos los archivos PDF y los convertimos en una lista de documentos.\n",
        "# Cada documento representa una p√°gina del PDF.\n",
        "docs = sum([ld.load() for ld in loaders], [])\n",
        "print(f\"Se leyeron {len(docs)} p√°ginas de {len(loaders)} PDFs\")\n",
        "\n",
        "# Preparamos una herramienta para dividir los textos largos en fragmentos m√°s peque√±os.\n",
        "# Cada fragmento tendr√° hasta 768 caracteres, y los fragmentos se superponen 128 caracteres.\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=768, chunk_overlap=128)\n",
        "\n",
        "# Aplicamos la divisi√≥n a todos los documentos cargados.\n",
        "docs_split = splitter.split_documents(docs)\n",
        "print(f\"Total de fragmentos: {len(docs_split)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188,
          "referenced_widgets": [
            "aa2c4443ad59467b9f61558255e87409",
            "b31b92b66c4043ca8e343748f396c4ff",
            "2fbb81a3c32d493384f14ab40a7d07d0",
            "48096eabc04049fc913b77093031b6cc",
            "55604a449f564a0b80cc1fe3fce33576",
            "978815a2a99a4c78b14e46151c676cff",
            "7cfa49bdae2e4d8ebc72be927d8e7472",
            "992f55922285406b86418e98ba00c155",
            "643e240530b64a03800f0905ea4561ca",
            "86a25e1ecd1349019a5a47f57e60d0b2",
            "03365ff72f294617a9908e789a4f854b"
          ]
        },
        "id": "ZB6ACy74sM5R",
        "outputId": "0ae89516-83c2-48a7-8958-c3037cb5d49d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö°Ô∏è  Cargando embeddings **E5-base** en CPU (m√°s r√°pidos)‚Ä¶\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aa2c4443ad59467b9f61558255e87409",
              "version_major": 2,
              "version_minor": 0,
              "state": {}
            },
            "text/plain": [
              "Batches:   0%|          | 0/51 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ √çndice FAISS listo\n"
          ]
        }
      ],
      "source": [
        "# ---------------------------------------------------------------\n",
        "# 2. Embeddings + √≠ndice FAISS\n",
        "# ---------------------------------------------------------------\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# 2. Crear representaciones num√©ricas (embeddings) y construir un √≠ndice de b√∫squeda (FAISS)\n",
        "# ---------------------------------------------------------------\n",
        "\n",
        "# Importamos FAISS, que permite crear un √≠ndice para buscar informaci√≥n r√°pidamente.\n",
        "# En general FAISS funcionar√° como una BBDD de vectores, otras opciones pueden ser ChromaDB\n",
        "# Estas BBDD vectoriales (para el almacenamiento de embeddings), permiten busqueda por similitud y busquedas avanzadas.\n",
        "# Importamos SentenceTransformer, una herramienta para convertir texto en n√∫meros.\n",
        "# Importamos la clase base de Langchain para crear \"embeddings personalizados\".\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.embeddings.base import Embeddings\n",
        "\n",
        "# Definimos el nombre del modelo que se va a usar para transformar texto en n√∫meros.\n",
        "# Este modelo se llama 'all-MiniLM-L6-v2', es ligero pero bastante preciso.\n",
        "EMB_MODEL = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "\n",
        "\n",
        "emb_device = 'cpu'\n",
        "print('‚ö°Ô∏è  Cargando embeddings **E5-base** en CPU (m√°s r√°pidos)‚Ä¶')\n",
        "\n",
        "# Cargamos el modelo de embeddings en la CPU.\n",
        "emb_model = SentenceTransformer(EMB_MODEL, device=emb_device)\n",
        "\n",
        "# Creamos una clase personalizada llamada E5Embeddings.\n",
        "# Esta clase se usa para transformar textos en \"embeddings\" (n√∫meros que representan el significado del texto).\n",
        "class E5Embeddings(Embeddings):\n",
        "    def embed_documents(self, texts):\n",
        "        # Convierte una lista de textos en listas de n√∫meros.\n",
        "        return emb_model.encode(\n",
        "            texts,\n",
        "            batch_size=32,                  # Procesa 32 textos a la vez\n",
        "            show_progress_bar=True,         # Muestra una barra de progreso\n",
        "            normalize_embeddings=True       # Normaliza los vectores para compararlos\n",
        "        ).tolist()\n",
        "\n",
        "    def embed_query(self, text):\n",
        "        # Convierte una sola pregunta o frase en una lista de n√∫meros.\n",
        "        return emb_model.encode(\n",
        "            text,\n",
        "            show_progress_bar=False,        # No muestra progreso porque es solo una frase\n",
        "            normalize_embeddings=True\n",
        "        ).tolist()\n",
        "\n",
        "\n",
        "embeddings = E5Embeddings()\n",
        "\n",
        "# Creamos un √≠ndice de b√∫squeda usando FAISS.\n",
        "# Este √≠ndice permite hacer b√∫squedas r√°pidas dentro de los fragmentos de texto previamente creados.\n",
        "faiss_index = FAISS.from_documents(docs_split, embeddings)\n",
        "\n",
        "print('‚úÖ √çndice FAISS listo')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDDBEbRnsSRi",
        "outputId": "61977c21-75c0-47fa-bb96-3c0fb77b9405"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ LLM cargado\n"
          ]
        }
      ],
      "source": [
        "# ---------------------------------------------------------------\n",
        "# 3. Cargar LLM\n",
        "# ---------------------------------------------------------------\n",
        "\n",
        "# Importamos herramientas de Hugging Face:\n",
        "# - AutoTokenizer: convierte texto en un formato que el modelo puede entender.\n",
        "# - AutoModelForSeq2SeqLM: es el modelo de lenguaje que puede generar o transformar texto.\n",
        "# Tambi√©n importamos HuggingFacePipeline, que permite usar estos modelos dentro del flujo de Langchain.\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "\n",
        "# Elegimos un modelo llamado \"google/flan-t5-base\".\n",
        "# Este modelo es relativamente peque√±o (250 MB) y funciona bien en una computadora sin tarjeta gr√°fica (CPU).\n",
        "LLM_ID = \"google/flan-t5-base\"\n",
        "\n",
        "# Cargamos el \"tokenizer\", que es como un traductor que prepara el texto para que el modelo lo entienda.\n",
        "tokenizer = AutoTokenizer.from_pretrained(LLM_ID)\n",
        "\n",
        "# Cargamos el modelo de lenguaje ya entrenado, listo para usar.\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(LLM_ID)  # Se mantendr√° funcionando en CPU\n",
        "\n",
        "# Creamos un \"pipeline\", que es una forma f√°cil de usar el modelo para generar texto.\n",
        "text_gen = pipeline(\n",
        "    \"text2text-generation\",     # Indicamos que vamos a generar texto a partir de texto\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=128,         # El texto generado tendr√° como m√°ximo 128 palabras nuevas\n",
        "    do_sample=True,             # Activamos la aleatoriedad en las respuestas (m√°s variado)\n",
        "    temperature=0.7,            # Controla qu√© tan creativas son las respuestas (0.7 es un balance)\n",
        "    top_p=0.9                   # Limita las palabras posibles a las m√°s probables (para mantener coherencia)\n",
        ")\n",
        "\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=text_gen)\n",
        "print(\"‚úÖ LLM cargado\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Bd0jBT9GsVyj",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "7c384a60-729b-49b1-8e1a-a7bbe60b6aee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cadena RAG inicializada\n"
          ]
        }
      ],
      "source": [
        "# ---------------------------------------------------------------\n",
        "# 4. RAG con LangChain\n",
        "# ---------------------------------------------------------------\n",
        "\n",
        "# La arquitectura RAG (generaci√≥n aumentada por recuperaci√≥n), nos permite optimizar la\n",
        "# salida de un Modelo de lenguaje, de forma que su respuesta, haga referencia a una\n",
        "# base de concimientos especifica. Pr√°cticamente, se encarga de extender las capacidades\n",
        "# de un LLM de responder sobre informaci√≥n especifca.\n",
        "\n",
        "# Este proceso se puede ver de la siguiente forma:\n",
        "# 1.- Recibimos el query del usuario\n",
        "# 2.- Recuperamos informaci√≥n relevante del query del usuario (retriever) (informaci√≥n previamente dividida y sus embbedings)\n",
        "# 3.- Aumentamos el query del usuario con la informaci√≥n relevante y enviamos el query (prompt) al LLM\n",
        "# 4.- Regresamos al usuario la respuesta del LLM.\n",
        "\n",
        "# Importamos herramientas necesarias de LangChain:\n",
        "# - RetrievalQA: permite hacer preguntas y obtener respuestas usando tanto un modelo de lenguaje como un buscador de informaci√≥n.\n",
        "# - StreamingStdOutCallbackHandler: sirve para imprimir las respuestas poco a poco mientras se generan.\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "\n",
        "# Creamos un \"streamer\" que mostrar√° el texto en pantalla mientras se genera, en tiempo real.\n",
        "streamer = StreamingStdOutCallbackHandler()\n",
        "\n",
        "# Creamos un \"retriever\" (buscador) a partir del √≠ndice FAISS creado antes.\n",
        "# Este buscar√° los fragmentos m√°s relevantes para responder una pregunta.\n",
        "# \"k=1\" significa que solo se recuperar√° **un fragmento de texto** por pregunta.\n",
        "# Esto se hace porque el modelo que estamos usando no puede manejar textos muy largos (solo 512 tokens).\n",
        "retriever = faiss_index.as_retriever(search_kwargs={\"k\": 1}) # muy importante, k=1 ya que el modelo utilizado tiene l√≠mite de 512 tokens, esto produce resultados \"meh\", con mejor modelo, mejores resultados, se elige este modelo debido a que queremos probar todo \"local\"\n",
        "\n",
        "# Creamos una cadena QA que combina:\n",
        "# - El modelo de lenguaje que ya cargamos (llm)\n",
        "# - El \"retriever\" que busca el contenido m√°s relevante\n",
        "# - Una estrategia llamada \"stuff\" que simplemente pasa el texto encontrado tal cual al modelo\n",
        "# - La opci√≥n de devolver tambi√©n el texto fuente (√∫til para saber de d√≥nde sali√≥ la respuesta)\n",
        "# - El \"streamer\" para mostrar la respuesta mientras se genera\n",
        "# - verbose=True para ver informaci√≥n detallada del proceso\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type='stuff',\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True,\n",
        "    callbacks=[streamer],\n",
        "    verbose=True\n",
        ")\n",
        "print('Cadena RAG inicializada')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVyr9_GUdysZ"
      },
      "source": [
        "# Chatbot, incluyendo ejemplos de prueba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 767
        },
        "id": "QBVooU2PVh3k",
        "outputId": "9e75603e-ed3b-4133-d816-8a8773b2f11e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://1e37ea4cc4fd805396.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://1e37ea4cc4fd805396.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "¬øQui√©n publica las NOM?\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{'query': '¬øQui√©n publica las NOM?', 'result': 'El Comit√© Consultivo Nacional de Normalizaci √≥n de Innovaci√≥n, Desarrollo, Tecnologas e Informaci√≥n en Salud', 'source_documents': [Document(id='34554267-3103-4aba-8e6a-13a59a41e9b4', metadata={'producer': 'Microsoft¬Æ Word 2013', 'creator': 'Microsoft¬Æ Word 2013', 'creationdate': '2017-02-21T08:44:09-06:00', 'title': '', 'author': 'DOF', 'moddate': '2017-02-21T08:44:09-06:00', 'source': '/content/drive/MyDrive/2025-1/NOM_PDFs/037ssa32017.pdf', 'total_pages': 13, 'page': 0, 'page_label': '1'}, page_content='Que durante el periodo de cons ulta p√∫blica, que concluy√≥ el 1o.  de noviembre del  2014, fueron recibidos \\nen la sede del citado Comit√©, los comentarios formulados por los interesados respecto del proyecto de la \\nNorma Oficial Mexicana, raz√≥n por la cual, con fecha previa fueron publicadas en el Diario Oficial de  la \\nFederaci√≥n las respue stas a los mismos, en t√©rminos de lo previsto por el art√≠culo 47, fracci√≥n III, de la Ley \\nFederal sobre Metrolog√≠a y Normalizaci√≥n, y \\nQue en atenci√≥n a las anteriores consideraciones, contando con la aprobaci√≥n del Comit√© Consultivo \\nNacional de Normalizaci √≥n de Innovaci√≥n, Desarrollo, Tecnolog√≠as e Informaci√≥n en Salud, he tenido a bien \\nexpedir y ordenar la publicaci√≥n de:')]}\n",
            "El Comit√© Consultivo Nacional de Normalizaci √≥n de Innovaci√≥n, Desarrollo, Tecnologas e Informaci√≥n en Salud\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "print(llm(\"Hola\", max_new_tokens=8))\n",
        "\n",
        "# Definimos una funci√≥n que manejar√° la interacci√≥n del usuario con el chatbot.\n",
        "# Esta funci√≥n recibe una pregunta, busca la respuesta con RAG y muestra la fuente.\n",
        "def rag_chat(text):\n",
        "    print(text)  # Imprime la pregunta en la consola.\n",
        "\n",
        "    # Usa la cadena RAG (b√∫squeda + generaci√≥n) para obtener una respuesta basada en los PDFs\n",
        "    out = qa_chain({'query': text})\n",
        "    print(out)\n",
        "\n",
        "    answer = out['result']\n",
        "    print(answer)\n",
        "\n",
        "    # Extrae los nombres de los archivos PDF que sirvieron como fuente de la respuesta.\n",
        "    # Elimina duplicados y los ordena alfab√©ticamente.\n",
        "    srcs = '\\n'.join(sorted({\n",
        "        os.path.basename(d.metadata['source'])\n",
        "        for d in out['source_documents']\n",
        "    }))\n",
        "    return f\"{answer}\\n\\nüìÑ **Fuentes consultadas:**\\n{srcs}\"\n",
        "\n",
        "# Creamos una interfaz gr√°fica con Gradio\n",
        "demo = gr.Interface(\n",
        "    fn=rag_chat,  # Funci√≥n que se ejecuta cuando el usuario hace una pregunta\n",
        "    inputs=gr.Textbox(lines=2, label='Pregunta', value=\"¬øQui√©n publica las NOM?\"),  # Entrada de texto con una pregunta inicial\n",
        "    outputs=gr.Markdown(label='Respuesta'),  # Salida en formato de texto con estilo Markdown\n",
        "    title='Chatbot RAG sobre Normas Oficiales Mexicanas',\n",
        "    description='Ingresa una pregunta y obt√©n una respuesta fundamentada en los PDFs de las NOM.'\n",
        ")\n",
        "\n",
        "demo.queue()\n",
        "demo.launch(share=True, debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kx-dZSFJz9cK"
      },
      "source": [
        "# **Conclusiones:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w3usdaC0BCj"
      },
      "source": [
        "**El RAG marca la diferencia**\n",
        "\n",
        "Usar recuperaci√≥n de fragmentos result√≥ clave. El chatbot no ‚Äúinventa‚Äù requisitos. Muestra el p√°rrafo exacto de la NOM y luego lo explica en lenguaje claro. Esto genera confianza en el usuario. El uso de RAG permite que el modelo de informaci√≥n actualizada y relevante al contexto espec√≠fico.\n",
        "\n",
        "La posibilidad de aumentar los datos con cualquier archivo hace que el modelo sea pertinente al responder a una mayor variedad de prompts. Adem√°s de que el modelo sea portable a m√°s casos espec√≠ficos al no estar ligado a esta informaci√≥n externa.\n",
        "\n",
        "**Un solo punto de consulta ahorra tiempo**\n",
        "\n",
        "Antes el personal deb√≠a abrir varios PDF y buscar a mano. Ahora obtiene la respuesta en menos de un minuto. Esto mejora la eficiencia durante auditor√≠as internas y capacitaciones.\n",
        "\n",
        "**Mdelos m√°s grandes dan mejores respuestas**\n",
        "\n",
        "Con un modelo peque√±o las respuestas son limitadas. La calidad del LLM influye mucho cuando la pregunta es compleja.\n",
        "\n",
        "\n",
        "**L√≠mites actuales**\n",
        "\n",
        "* No detecta si la NOM fue derogada o modificada despu√©s de la fecha de corte.\n",
        "\n",
        "* No responde consultas que mezclan normativa con procedimientos cl√≠nicos que no est√°n en las NOM.\n",
        "\n",
        "\n",
        "En conjunto, la actividad mostr√≥ que un chatbot LLM + RAG es una soluci√≥n viable para acercar la normativa de salud al personal operativo. Se logr√≥ reducir la carga de b√∫squeda, aumentar la claridad de la informaci√≥n y sentar las bases para mejoras futuras.\n",
        "\n",
        "Este prototipo demuestra un camino claro hacia herramientas m√°s robustas, capaces de integrarse con fuentes normativas actualizadas y expandirse a nuevas √°reas tem√°ticas. Con mejoras progresivas en el modelo y la cobertura documental, esta soluci√≥n puede convertirse en un eje estrat√©gico para fortalecer el cumplimiento, la formaci√≥n continua y la toma de decisiones basadas en evidencia normativa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtB5Q3m41YQ0"
      },
      "source": [
        "# **Fin de la actividad chatbot: LLM + RAG**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "state": {},
        "03365ff72f294617a9908e789a4f854b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2fbb81a3c32d493384f14ab40a7d07d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_992f55922285406b86418e98ba00c155",
            "max": 51,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_643e240530b64a03800f0905ea4561ca",
            "value": 51
          }
        },
        "48096eabc04049fc913b77093031b6cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86a25e1ecd1349019a5a47f57e60d0b2",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_03365ff72f294617a9908e789a4f854b",
            "value": "‚Äá51/51‚Äá[03:28&lt;00:00,‚Äá‚Äá2.00s/it]"
          }
        },
        "55604a449f564a0b80cc1fe3fce33576": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "643e240530b64a03800f0905ea4561ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7cfa49bdae2e4d8ebc72be927d8e7472": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "86a25e1ecd1349019a5a47f57e60d0b2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "978815a2a99a4c78b14e46151c676cff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "992f55922285406b86418e98ba00c155": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa2c4443ad59467b9f61558255e87409": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b31b92b66c4043ca8e343748f396c4ff",
              "IPY_MODEL_2fbb81a3c32d493384f14ab40a7d07d0",
              "IPY_MODEL_48096eabc04049fc913b77093031b6cc"
            ],
            "layout": "IPY_MODEL_55604a449f564a0b80cc1fe3fce33576"
          }
        },
        "b31b92b66c4043ca8e343748f396c4ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_978815a2a99a4c78b14e46151c676cff",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7cfa49bdae2e4d8ebc72be927d8e7472",
            "value": "Batches:‚Äá100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
