{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hVND8xY2OKY"
      },
      "source": [
        "# **Procesamiento de Lenguaje Natural**\n",
        "\n",
        "## Maestría en Inteligencia Artificial Aplicada\n",
        "#### Tecnológico de Monterrey\n",
        "#### Prof Luis Eduardo Falcón Morales\n",
        "\n",
        "### **Actividad en Equipos: sistema LLM + RAG**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aimHVFOv23lm"
      },
      "source": [
        "* **Nombres y matrículas:**\n",
        "\n",
        "  *   Arturo Jain Delgadillo A01794992\n",
        "  *   Ramiro Martin Jaramillo Romero A01171251\n",
        "  * José Ashamat Jaimes Saavedra A01736690\n",
        "  *   Owen Jáuregui Borbón A01638122\n",
        "\n",
        "* **Número de Equipo:**\n",
        "  *   Equipo 80\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jimvsiVgjMg"
      },
      "source": [
        "* ##### **El formato de este cuaderno de Jupyter es libre, pero debe incuir al menos las siguientes secciones:**\n",
        "\n",
        "  * ##### **Introducción de la problemática a resolver.**\n",
        "  * ##### **Sistema RAG + LLM**\n",
        "  * ##### **El chatbot, incluyendo ejemplos de prueba.**\n",
        "  * ##### **Conclusiones**\n",
        "\n",
        "* ##### **Pueden importar los paquetes o librerías que requieran.**\n",
        "\n",
        "* ##### **Pueden incluir las celdas y líneas de código que deseen.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22lMsIlJUSoy"
      },
      "source": [
        "# Introducción de la problemática a resolver."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SieQLZn0GJ3I"
      },
      "source": [
        "El personal de salud en México debe cumplir con muchas Normas Oficiales Mexicanas (NOM). Estas normas cubren higiene, bioseguridad, infraestructura y calidad. Incumplirlas puede traer multas, clausuras e incluso poner en riesgo a los pacientes.\n",
        "\n",
        "Hay tres problemas frecuentes:\n",
        "\n",
        "* Encontrar la norma correcta entre docenas de documentos y versiones.\n",
        "\n",
        "* Entender el lenguaje jurídico y técnico y convertirlo en pasos claros para el día a día.\n",
        "\n",
        "* Seguir las actualizaciones que publica el Diario Oficial.\n",
        "\n",
        "Clínicas pequeñas, laboratorios y comités internos de calidad suelen trabajar sin apoyo legal especializado. Esto provoca retrasos en auditorías y riesgos de sanción.\n",
        "\n",
        "Para cerrar esa brecha proponemos un chatbot que combina un modelo de lenguaje grande con RAG. El sistema carga 21 NOM de salud (por ejemplo la NOM-007 sobre atención obstétrica y la NOM-059 sobre buenas prácticas de fabricación). Cuando alguien hace una pregunta en lenguaje natural, el chatbot busca los párrafos más cercanos y genera una respuesta en español con la cita original.\n",
        "\n",
        "Así el personal puede:\n",
        "\n",
        "* Consultar requisitos concretos.\n",
        "\n",
        "* Recibir explicaciones sencillas con la referencia al texto oficial.\n",
        "\n",
        "* Acceder a la versión vigente sin buscar archivos PDF.\n",
        "\n",
        "Con esto mejoramos el acceso a la normativa, reducimos errores de cumplimiento y apoyamos la capacitación continua en los servicios de salud."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQ_N7r7hUYfy"
      },
      "source": [
        "# Sistema RAG + LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bfs5Zxc9j7Uf",
        "outputId": "45ab47d4-c4cb-472b-ed0a-e5cf3e715395"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: transformers 4.53.0.dev0\n",
            "Uninstalling transformers-4.53.0.dev0:\n",
            "  Successfully uninstalled transformers-4.53.0.dev0\n",
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-5bcylz79\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-5bcylz79\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit 2166b6b4ff09f6dd3867ab982f262f66482aa968\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0.dev0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0.dev0) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0.dev0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0.dev0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0.dev0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0.dev0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0.dev0) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0.dev0) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0.dev0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.53.0.dev0) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.53.0.dev0) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.53.0.dev0) (1.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.53.0.dev0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.53.0.dev0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.53.0.dev0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.53.0.dev0) (2025.6.15)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.53.0.dev0-py3-none-any.whl size=11461635 sha256=d04cab7673c5f582d544a63ef9e5fce4649cb3838adf3ea5063dd8935d3c9d51\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-e8h2jcgl/wheels/32/4b/78/f195c684dd3a9ed21f3b39fe8f85b48df7918581b6437be143\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.52.4\n",
            "    Uninstalling transformers-4.52.4:\n",
            "      Successfully uninstalled transformers-4.52.4\n",
            "Successfully installed transformers-4.53.0.dev0\n"
          ]
        }
      ],
      "source": [
        "# Incluyan a continuación todas las celdas (de código o texto) que deseen...\n",
        "\n",
        "!pip uninstall -y transformers\n",
        "!pip -q install --upgrade \\\n",
        "  langchain langchain-community langchain-huggingface \\\n",
        "  faiss-cpu sentence-transformers \\\n",
        "  bitsandbytes accelerate \\\n",
        "  pypdf \"gradio>=4.17.0\"\n",
        "!pip install --no-cache-dir git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diYmiW9pNz32",
        "outputId": "2eb446e4-ea0d-44b7-bc71-20d355d33a12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os, warnings, gc, torch\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "DATA_DIR = '/content/drive/MyDrive/2025-1/NOM_PDFs'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPNz6VG2sNQN",
        "outputId": "e2868472-c424-4df6-d948-c97087cf1407"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Se leyeron 323 páginas de 21 PDFs\n",
            "Total de fragmentos: 1625\n"
          ]
        }
      ],
      "source": [
        "# ---------------------------------------------------------------\n",
        "# 1. Cargar y fragmentar los PDFs\n",
        "# ---------------------------------------------------------------\n",
        "\n",
        "# LangChain es un framework de trabajo que se esta posicionando como lider en la creación de arquitecturas/agentes\n",
        "# RAG, RAFT en forma de cadena (LangChain) y/o agentes, agentes multitooling, redes de agentes, multiagentes en\n",
        "# forma de grafo (LangGraph). Estos agentes en grafo, tienen la capacidad de elegir, con base en el query\n",
        "# sus acciones, tareas y/o pasos a seguir.\n",
        "# LangChain/LangGraph, nos brinda todas las herramientas para lograr estos procesos, e incluso\n",
        "# cuenta con LangSmith, que permite observar lo que pasa dentro del modelo (cuando esta el procesamiento)\n",
        "\n",
        "# RecursiveCharacterTextSplitter: sirve para dividir textos largos en partes más pequeñas.\n",
        "# Utilizamos el framework Langchain para cargar los documentos PDF y para separar los documentos finales.\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Obtenemos la lista de archivos en la carpeta DATA_DIR que terminan en \".pdf\"\n",
        "pdf_files = [f for f in os.listdir(DATA_DIR) if f.lower().endswith('.pdf')]\n",
        "\n",
        "# Si no se encontró ningún archivo PDF, mostramos un error.\n",
        "if not pdf_files:\n",
        "    raise ValueError(f\"No se encontraron PDFs en {DATA_DIR}.\")\n",
        "\n",
        "# Creamos una lista de \"lectores de PDF\", uno por cada archivo encontrado.\n",
        "loaders = [PyPDFLoader(os.path.join(DATA_DIR, f)) for f in pdf_files]\n",
        "\n",
        "# Leemos todos los archivos PDF y los convertimos en una lista de documentos.\n",
        "# Cada documento representa una página del PDF.\n",
        "docs = sum([ld.load() for ld in loaders], [])\n",
        "print(f\"Se leyeron {len(docs)} páginas de {len(loaders)} PDFs\")\n",
        "\n",
        "# Preparamos una herramienta para dividir los textos largos en fragmentos más pequeños.\n",
        "# Cada fragmento tendrá hasta 768 caracteres, y los fragmentos se superponen 128 caracteres.\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=768, chunk_overlap=128)\n",
        "\n",
        "# Aplicamos la división a todos los documentos cargados.\n",
        "docs_split = splitter.split_documents(docs)\n",
        "print(f\"Total de fragmentos: {len(docs_split)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188,
          "referenced_widgets": [
            "aa2c4443ad59467b9f61558255e87409",
            "b31b92b66c4043ca8e343748f396c4ff",
            "2fbb81a3c32d493384f14ab40a7d07d0",
            "48096eabc04049fc913b77093031b6cc",
            "55604a449f564a0b80cc1fe3fce33576",
            "978815a2a99a4c78b14e46151c676cff",
            "7cfa49bdae2e4d8ebc72be927d8e7472",
            "992f55922285406b86418e98ba00c155",
            "643e240530b64a03800f0905ea4561ca",
            "86a25e1ecd1349019a5a47f57e60d0b2",
            "03365ff72f294617a9908e789a4f854b"
          ]
        },
        "id": "ZB6ACy74sM5R",
        "outputId": "0ae89516-83c2-48a7-8958-c3037cb5d49d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⚡️  Cargando embeddings **E5-base** en CPU (más rápidos)…\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aa2c4443ad59467b9f61558255e87409",
              "version_major": 2,
              "version_minor": 0,
              "state": {}
            },
            "text/plain": [
              "Batches:   0%|          | 0/51 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Índice FAISS listo\n"
          ]
        }
      ],
      "source": [
        "# ---------------------------------------------------------------\n",
        "# 2. Embeddings + índice FAISS\n",
        "# ---------------------------------------------------------------\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# 2. Crear representaciones numéricas (embeddings) y construir un índice de búsqueda (FAISS)\n",
        "# ---------------------------------------------------------------\n",
        "\n",
        "# Importamos FAISS, que permite crear un índice para buscar información rápidamente.\n",
        "# En general FAISS funcionará como una BBDD de vectores, otras opciones pueden ser ChromaDB\n",
        "# Estas BBDD vectoriales (para el almacenamiento de embeddings), permiten busqueda por similitud y busquedas avanzadas.\n",
        "# Importamos SentenceTransformer, una herramienta para convertir texto en números.\n",
        "# Importamos la clase base de Langchain para crear \"embeddings personalizados\".\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.embeddings.base import Embeddings\n",
        "\n",
        "# Definimos el nombre del modelo que se va a usar para transformar texto en números.\n",
        "# Este modelo se llama 'all-MiniLM-L6-v2', es ligero pero bastante preciso.\n",
        "EMB_MODEL = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "\n",
        "\n",
        "emb_device = 'cpu'\n",
        "print('⚡️  Cargando embeddings **E5-base** en CPU (más rápidos)…')\n",
        "\n",
        "# Cargamos el modelo de embeddings en la CPU.\n",
        "emb_model = SentenceTransformer(EMB_MODEL, device=emb_device)\n",
        "\n",
        "# Creamos una clase personalizada llamada E5Embeddings.\n",
        "# Esta clase se usa para transformar textos en \"embeddings\" (números que representan el significado del texto).\n",
        "class E5Embeddings(Embeddings):\n",
        "    def embed_documents(self, texts):\n",
        "        # Convierte una lista de textos en listas de números.\n",
        "        return emb_model.encode(\n",
        "            texts,\n",
        "            batch_size=32,                  # Procesa 32 textos a la vez\n",
        "            show_progress_bar=True,         # Muestra una barra de progreso\n",
        "            normalize_embeddings=True       # Normaliza los vectores para compararlos\n",
        "        ).tolist()\n",
        "\n",
        "    def embed_query(self, text):\n",
        "        # Convierte una sola pregunta o frase en una lista de números.\n",
        "        return emb_model.encode(\n",
        "            text,\n",
        "            show_progress_bar=False,        # No muestra progreso porque es solo una frase\n",
        "            normalize_embeddings=True\n",
        "        ).tolist()\n",
        "\n",
        "\n",
        "embeddings = E5Embeddings()\n",
        "\n",
        "# Creamos un índice de búsqueda usando FAISS.\n",
        "# Este índice permite hacer búsquedas rápidas dentro de los fragmentos de texto previamente creados.\n",
        "faiss_index = FAISS.from_documents(docs_split, embeddings)\n",
        "\n",
        "print('✅ Índice FAISS listo')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDDBEbRnsSRi",
        "outputId": "61977c21-75c0-47fa-bb96-3c0fb77b9405"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ LLM cargado\n"
          ]
        }
      ],
      "source": [
        "# ---------------------------------------------------------------\n",
        "# 3. Cargar LLM\n",
        "# ---------------------------------------------------------------\n",
        "\n",
        "# Importamos herramientas de Hugging Face:\n",
        "# - AutoTokenizer: convierte texto en un formato que el modelo puede entender.\n",
        "# - AutoModelForSeq2SeqLM: es el modelo de lenguaje que puede generar o transformar texto.\n",
        "# También importamos HuggingFacePipeline, que permite usar estos modelos dentro del flujo de Langchain.\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "\n",
        "# Elegimos un modelo llamado \"google/flan-t5-base\".\n",
        "# Este modelo es relativamente pequeño (250 MB) y funciona bien en una computadora sin tarjeta gráfica (CPU).\n",
        "LLM_ID = \"google/flan-t5-base\"\n",
        "\n",
        "# Cargamos el \"tokenizer\", que es como un traductor que prepara el texto para que el modelo lo entienda.\n",
        "tokenizer = AutoTokenizer.from_pretrained(LLM_ID)\n",
        "\n",
        "# Cargamos el modelo de lenguaje ya entrenado, listo para usar.\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(LLM_ID)  # Se mantendrá funcionando en CPU\n",
        "\n",
        "# Creamos un \"pipeline\", que es una forma fácil de usar el modelo para generar texto.\n",
        "text_gen = pipeline(\n",
        "    \"text2text-generation\",     # Indicamos que vamos a generar texto a partir de texto\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=128,         # El texto generado tendrá como máximo 128 palabras nuevas\n",
        "    do_sample=True,             # Activamos la aleatoriedad en las respuestas (más variado)\n",
        "    temperature=0.7,            # Controla qué tan creativas son las respuestas (0.7 es un balance)\n",
        "    top_p=0.9                   # Limita las palabras posibles a las más probables (para mantener coherencia)\n",
        ")\n",
        "\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=text_gen)\n",
        "print(\"✅ LLM cargado\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Bd0jBT9GsVyj",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "7c384a60-729b-49b1-8e1a-a7bbe60b6aee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cadena RAG inicializada\n"
          ]
        }
      ],
      "source": [
        "# ---------------------------------------------------------------\n",
        "# 4. RAG con LangChain\n",
        "# ---------------------------------------------------------------\n",
        "\n",
        "# La arquitectura RAG (generación aumentada por recuperación), nos permite optimizar la\n",
        "# salida de un Modelo de lenguaje, de forma que su respuesta, haga referencia a una\n",
        "# base de concimientos especifica. Prácticamente, se encarga de extender las capacidades\n",
        "# de un LLM de responder sobre información especifca.\n",
        "\n",
        "# Este proceso se puede ver de la siguiente forma:\n",
        "# 1.- Recibimos el query del usuario\n",
        "# 2.- Recuperamos información relevante del query del usuario (retriever) (información previamente dividida y sus embbedings)\n",
        "# 3.- Aumentamos el query del usuario con la información relevante y enviamos el query (prompt) al LLM\n",
        "# 4.- Regresamos al usuario la respuesta del LLM.\n",
        "\n",
        "# Importamos herramientas necesarias de LangChain:\n",
        "# - RetrievalQA: permite hacer preguntas y obtener respuestas usando tanto un modelo de lenguaje como un buscador de información.\n",
        "# - StreamingStdOutCallbackHandler: sirve para imprimir las respuestas poco a poco mientras se generan.\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "\n",
        "# Creamos un \"streamer\" que mostrará el texto en pantalla mientras se genera, en tiempo real.\n",
        "streamer = StreamingStdOutCallbackHandler()\n",
        "\n",
        "# Creamos un \"retriever\" (buscador) a partir del índice FAISS creado antes.\n",
        "# Este buscará los fragmentos más relevantes para responder una pregunta.\n",
        "# \"k=1\" significa que solo se recuperará **un fragmento de texto** por pregunta.\n",
        "# Esto se hace porque el modelo que estamos usando no puede manejar textos muy largos (solo 512 tokens).\n",
        "retriever = faiss_index.as_retriever(search_kwargs={\"k\": 1}) # muy importante, k=1 ya que el modelo utilizado tiene límite de 512 tokens, esto produce resultados \"meh\", con mejor modelo, mejores resultados, se elige este modelo debido a que queremos probar todo \"local\"\n",
        "\n",
        "# Creamos una cadena QA que combina:\n",
        "# - El modelo de lenguaje que ya cargamos (llm)\n",
        "# - El \"retriever\" que busca el contenido más relevante\n",
        "# - Una estrategia llamada \"stuff\" que simplemente pasa el texto encontrado tal cual al modelo\n",
        "# - La opción de devolver también el texto fuente (útil para saber de dónde salió la respuesta)\n",
        "# - El \"streamer\" para mostrar la respuesta mientras se genera\n",
        "# - verbose=True para ver información detallada del proceso\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type='stuff',\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True,\n",
        "    callbacks=[streamer],\n",
        "    verbose=True\n",
        ")\n",
        "print('Cadena RAG inicializada')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVyr9_GUdysZ"
      },
      "source": [
        "# Chatbot, incluyendo ejemplos de prueba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 767
        },
        "id": "QBVooU2PVh3k",
        "outputId": "9e75603e-ed3b-4133-d816-8a8773b2f11e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola, Hola\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://1e37ea4cc4fd805396.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://1e37ea4cc4fd805396.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "¿Quién publica las NOM?\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{'query': '¿Quién publica las NOM?', 'result': 'El Comité Consultivo Nacional de Normalizaci ón de Innovación, Desarrollo, Tecnologas e Información en Salud', 'source_documents': [Document(id='34554267-3103-4aba-8e6a-13a59a41e9b4', metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2017-02-21T08:44:09-06:00', 'title': '', 'author': 'DOF', 'moddate': '2017-02-21T08:44:09-06:00', 'source': '/content/drive/MyDrive/2025-1/NOM_PDFs/037ssa32017.pdf', 'total_pages': 13, 'page': 0, 'page_label': '1'}, page_content='Que durante el periodo de cons ulta pública, que concluyó el 1o.  de noviembre del  2014, fueron recibidos \\nen la sede del citado Comité, los comentarios formulados por los interesados respecto del proyecto de la \\nNorma Oficial Mexicana, razón por la cual, con fecha previa fueron publicadas en el Diario Oficial de  la \\nFederación las respue stas a los mismos, en términos de lo previsto por el artículo 47, fracción III, de la Ley \\nFederal sobre Metrología y Normalización, y \\nQue en atención a las anteriores consideraciones, contando con la aprobación del Comité Consultivo \\nNacional de Normalizaci ón de Innovación, Desarrollo, Tecnologías e Información en Salud, he tenido a bien \\nexpedir y ordenar la publicación de:')]}\n",
            "El Comité Consultivo Nacional de Normalizaci ón de Innovación, Desarrollo, Tecnologas e Información en Salud\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "print(llm(\"Hola\", max_new_tokens=8))\n",
        "\n",
        "# Definimos una función que manejará la interacción del usuario con el chatbot.\n",
        "# Esta función recibe una pregunta, busca la respuesta con RAG y muestra la fuente.\n",
        "def rag_chat(text):\n",
        "    print(text)  # Imprime la pregunta en la consola.\n",
        "\n",
        "    # Usa la cadena RAG (búsqueda + generación) para obtener una respuesta basada en los PDFs\n",
        "    out = qa_chain({'query': text})\n",
        "    print(out)\n",
        "\n",
        "    answer = out['result']\n",
        "    print(answer)\n",
        "\n",
        "    # Extrae los nombres de los archivos PDF que sirvieron como fuente de la respuesta.\n",
        "    # Elimina duplicados y los ordena alfabéticamente.\n",
        "    srcs = '\\n'.join(sorted({\n",
        "        os.path.basename(d.metadata['source'])\n",
        "        for d in out['source_documents']\n",
        "    }))\n",
        "    return f\"{answer}\\n\\n📄 **Fuentes consultadas:**\\n{srcs}\"\n",
        "\n",
        "# Creamos una interfaz gráfica con Gradio\n",
        "demo = gr.Interface(\n",
        "    fn=rag_chat,  # Función que se ejecuta cuando el usuario hace una pregunta\n",
        "    inputs=gr.Textbox(lines=2, label='Pregunta', value=\"¿Quién publica las NOM?\"),  # Entrada de texto con una pregunta inicial\n",
        "    outputs=gr.Markdown(label='Respuesta'),  # Salida en formato de texto con estilo Markdown\n",
        "    title='Chatbot RAG sobre Normas Oficiales Mexicanas',\n",
        "    description='Ingresa una pregunta y obtén una respuesta fundamentada en los PDFs de las NOM.'\n",
        ")\n",
        "\n",
        "demo.queue()\n",
        "demo.launch(share=True, debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kx-dZSFJz9cK"
      },
      "source": [
        "# **Conclusiones:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w3usdaC0BCj"
      },
      "source": [
        "**El RAG marca la diferencia**\n",
        "\n",
        "Usar recuperación de fragmentos resultó clave. El chatbot no “inventa” requisitos. Muestra el párrafo exacto de la NOM y luego lo explica en lenguaje claro. Esto genera confianza en el usuario. El uso de RAG permite que el modelo de información actualizada y relevante al contexto específico.\n",
        "\n",
        "La posibilidad de aumentar los datos con cualquier archivo hace que el modelo sea pertinente al responder a una mayor variedad de prompts. Además de que el modelo sea portable a más casos específicos al no estar ligado a esta información externa.\n",
        "\n",
        "**Un solo punto de consulta ahorra tiempo**\n",
        "\n",
        "Antes el personal debía abrir varios PDF y buscar a mano. Ahora obtiene la respuesta en menos de un minuto. Esto mejora la eficiencia durante auditorías internas y capacitaciones.\n",
        "\n",
        "**Mdelos más grandes dan mejores respuestas**\n",
        "\n",
        "Con un modelo pequeño las respuestas son limitadas. La calidad del LLM influye mucho cuando la pregunta es compleja.\n",
        "\n",
        "\n",
        "**Límites actuales**\n",
        "\n",
        "* No detecta si la NOM fue derogada o modificada después de la fecha de corte.\n",
        "\n",
        "* No responde consultas que mezclan normativa con procedimientos clínicos que no están en las NOM.\n",
        "\n",
        "\n",
        "En conjunto, la actividad mostró que un chatbot LLM + RAG es una solución viable para acercar la normativa de salud al personal operativo. Se logró reducir la carga de búsqueda, aumentar la claridad de la información y sentar las bases para mejoras futuras.\n",
        "\n",
        "Este prototipo demuestra un camino claro hacia herramientas más robustas, capaces de integrarse con fuentes normativas actualizadas y expandirse a nuevas áreas temáticas. Con mejoras progresivas en el modelo y la cobertura documental, esta solución puede convertirse en un eje estratégico para fortalecer el cumplimiento, la formación continua y la toma de decisiones basadas en evidencia normativa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtB5Q3m41YQ0"
      },
      "source": [
        "# **Fin de la actividad chatbot: LLM + RAG**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "state": {},
        "03365ff72f294617a9908e789a4f854b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2fbb81a3c32d493384f14ab40a7d07d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_992f55922285406b86418e98ba00c155",
            "max": 51,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_643e240530b64a03800f0905ea4561ca",
            "value": 51
          }
        },
        "48096eabc04049fc913b77093031b6cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86a25e1ecd1349019a5a47f57e60d0b2",
            "placeholder": "​",
            "style": "IPY_MODEL_03365ff72f294617a9908e789a4f854b",
            "value": " 51/51 [03:28&lt;00:00,  2.00s/it]"
          }
        },
        "55604a449f564a0b80cc1fe3fce33576": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "643e240530b64a03800f0905ea4561ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7cfa49bdae2e4d8ebc72be927d8e7472": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "86a25e1ecd1349019a5a47f57e60d0b2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "978815a2a99a4c78b14e46151c676cff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "992f55922285406b86418e98ba00c155": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa2c4443ad59467b9f61558255e87409": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b31b92b66c4043ca8e343748f396c4ff",
              "IPY_MODEL_2fbb81a3c32d493384f14ab40a7d07d0",
              "IPY_MODEL_48096eabc04049fc913b77093031b6cc"
            ],
            "layout": "IPY_MODEL_55604a449f564a0b80cc1fe3fce33576"
          }
        },
        "b31b92b66c4043ca8e343748f396c4ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_978815a2a99a4c78b14e46151c676cff",
            "placeholder": "​",
            "style": "IPY_MODEL_7cfa49bdae2e4d8ebc72be927d8e7472",
            "value": "Batches: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
